{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outline**\n",
    "\n",
    "- Introduction to GAI and its application\n",
    "- Generative Model Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generative AI: A Year of Transformation**\n",
    "\n",
    "<img src=\"./images/GAI1.png\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "- Generative AI has gained extensive attention and investment in the past year\n",
    "  - It can produce coherent text, images, code, and beyond-impressive outputs with just a simple textual prompt\n",
    "- Generative AI goes beyond typical natural language processing (NLP) tasks\n",
    "\n",
    "- Countless use cases:\n",
    "  - Explaining complex algorithms.\n",
    "  - Building bots.\n",
    "  - Assisting in app development.\n",
    "  - Explaining academic concepts.\n",
    "\n",
    "\n",
    "- Fields undergoing transformation:\n",
    "  - Animation.\n",
    "  - Gaming.\n",
    "  - Art.\n",
    "  - Movies.\n",
    "  - Architecture.\n",
    "  - Coffee Industry (?)\n",
    "\n",
    "#### The \"Aha!\" Moment in 2022\n",
    "\n",
    "- Important questions:\n",
    "  - Why now?\n",
    "  - What's next?\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What is Generative AI?**\n",
    "\n",
    "<img src=\"./images/GAI8.webp\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- Generative AI is a subfield of **machine learning**.\n",
    "- It involves training AI models on **real-world data**.\n",
    "- These models **generate new content** like text, images, and code.\n",
    "- Comparable to **what humans would create**.\n",
    "\n",
    "\n",
    "#### How Generative AI Works\n",
    "\n",
    "- Training algorithms on large datasets.\n",
    "  - Identifying and learning patterns.\n",
    "    - **Neural networks** learn these patterns.\n",
    "      - **Generate new data** following the learned patterns.\n",
    "\n",
    "#### Generative AI in Natural Language Processing (NLP)\n",
    "\n",
    "- Generative AI in NLP processes a vast corpus.\n",
    "  - Responds to prompts based on learned probabilities.\n",
    "    - Examples: Autocomplete and advanced models like ChatGPT and DALL-E.\n",
    "  - Utilizes different model architectures.\n",
    "\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "##  **General Components for Generative AI**\n",
    "\n",
    "1. **Input Data:**\n",
    "   - Generative AI models start with input data, which can be in various forms such as text, images, or structured data.\n",
    "\n",
    "2. **Encoder (Optional):**\n",
    "   - Some models use an encoder to transform input data into a suitable representation, especially in sequence-to-sequence models.\n",
    "\n",
    "3. **Generator (Decoder):**\n",
    "   - The core of the generative model is the generator or decoder, which generates new data based on learned patterns.\n",
    "   - This component typically consists of neural network layers.\n",
    "\n",
    "4. **Latent Space (Optional):**\n",
    "   - In certain models like VAEs and GANs, a latent space is used to represent data in a compressed form.\n",
    "   - The latent space is learned during training and can be sampled for generating new data.\n",
    "\n",
    "5. **Loss Function:**\n",
    "   - Generative models use a loss function to measure the difference between generated data and target data.\n",
    "   - The model aims to minimize this loss during training to improve its generative capabilities.\n",
    "\n",
    "6. **Training Data:**\n",
    "   - Generative models are trained on a dataset containing examples of the data they are supposed to generate.\n",
    "   - The model learns from this data to capture underlying patterns.\n",
    "\n",
    "7. **Optimizer:**\n",
    "   - Optimization algorithms like SGD or Adam are used to update the model's parameters during training to minimize the loss function.\n",
    "\n",
    "8. **Sampling:**\n",
    "   - Once trained, the model can generate new data by sampling from the learned distribution in the latent space or directly from the generator.\n",
    "\n",
    "9. **Output Data:**\n",
    "   - The generated data is the final output of the generative AI model, and it can be in the same or a different format as the input data, depending on the task.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **History of Generative AI**\n",
    "\n",
    "#### Early Generative AI - Eliza\n",
    "\n",
    "- Eliza chatbot developed in 1966 by Joseph Weizenbaum.\n",
    "- Early implementations used rules-based approaches.\n",
    "- Eliza had a limited vocabulary, lacked context, and overrelied on patterns.\n",
    "- These limitations led to frequent breakdowns.\n",
    "- **Challenges**\n",
    "  - Customization and expansion of early chatbots were challenging\n",
    "  - They struggled to adapt to different user inputs.\n",
    "  - Lack of context made meaningful conversations difficult.\n",
    "  - These limitations hindered their practical use.\n",
    "\n",
    "<img src=\"./images/ELIZA_conversation.png\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "#### **Recent Progress in Generative AI**\n",
    "\n",
    "- Key Factors in Recent Success\n",
    "  - Deep learning's three critical components:\n",
    "    - **Scaling models**: Larger and more complex architectures.\n",
    "    - **Large datasets**: Abundant real-world data for training.\n",
    "    - Increased **compute power**: Faster training and more sophisticated models.\n",
    "- These factors work together to drive the generative AI revolution.\n",
    "\n",
    "<img src=\"./images/whynow.png\" width=\"800\" align=\"center\"/>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GPUs and their Application to Machine Learning**\n",
    "-  GPUs vs. CPUs\n",
    "   - GPUs designed for parallel processing.\n",
    "   - Ideal for computationally intensive tasks.\n",
    "   - Thousands of smaller cores for simultaneous processing.\n",
    "   - Contrasts with CPUs focused on sequential processing.\n",
    "\n",
    "- GPUs excel in training deep neural networks.\n",
    "- **Parallelism** speeds up training significantly.\n",
    "- Enables the handling of large, complex networks.\n",
    "\n",
    "#### Beyond Training\n",
    "\n",
    "- GPUs not limited to training.\n",
    "- Used for inference and real-time applications.\n",
    "- Widely adopted in industries like healthcare, finance, and gaming.\n",
    "\n",
    "<img src=\"./images/Nvidia_CUDA_Logo.jpg\" width=\"500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AlexNet — 2012 — The Deep Learning Revolution**\n",
    "- Deep learning and CNNs led the charge.\n",
    "- CNNs (Convolutional Neural Networks) existed since the 1990s.\n",
    "  - Layers in a CNN\n",
    "    - Convolutional layers\n",
    "      - filters to detect various features.\n",
    "    - Pooling layers\n",
    "      - Reduce the spatial dimensions to preserve important information.\n",
    "    - Fully connected layers\n",
    "      - Often used for classification\n",
    "\n",
    "<img src=\"./images/CNN.jpeg\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "- Previously impractical due to intensive computing requirements.\n",
    "\n",
    "- https://poloclub.github.io/cnn-explainer/\n",
    "\n",
    "- In 2012, AlexNet emerged.\n",
    "  - A CNN model trained on GPUs and ImageNet data.\n",
    "    - An astonishing 11% performance gap with the runner-up!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transformers: Attention Is All You Need (Google) — 2017**\n",
    "\n",
    "- Deep learning lacked in natural language processing (NLP)\n",
    "- NLP not just about translation or classification\n",
    "- The challenge was coherent conversations with humans\n",
    "\n",
    "- **RNN (Recurrent Neural Network)**\n",
    "  - A type of neural network designed for sequential data.\n",
    "  - Processes data with loops, allowing information persistence.\n",
    "    - **How it works:**\n",
    "        - Takes input at each time step.\n",
    "        - Maintains a hidden state that captures previous information.\n",
    "\n",
    "- **LSTM (Long Short-Term Memory)**\n",
    "  - A type of RNN designed to address the vanishing gradient problem.\n",
    "  - Keeps long-term dependencies in sequential data.\n",
    "    - **How it works:**\n",
    "        - Similar to RNN but with specialized memory cells.\n",
    "        - Has gates (input, forget, output) to control information flow.\n",
    "        - Can store, read, and write information selectively.\n",
    "    - **Advantages:**\n",
    "        - Handles long-term dependencies effectively.\n",
    "        - Better at capturing and retaining sequential patterns.\n",
    "\n",
    "\n",
    "<img src=\"./images/RNNLSTM.png\"  width=\"400\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks were early staples in natural language processing and time series analysis.\n",
    "\n",
    "**Limitations**\n",
    "- Proficient at short sequences but struggled with **longer text**.\n",
    "- Couldn't capture complex ideas in extended text.\n",
    "\n",
    "## **Transformers**\n",
    "\n",
    "- Google introduced the \"Transformer\" model in 2017.\n",
    "- Presented in the groundbreaking paper \"Attention Is All You Need.\"\n",
    "- A milestone that revolutionized translation problems.\n",
    "\n",
    "\n",
    "#### The Power of Attention\n",
    "\n",
    "- \"Attention\" mechanism - a neural network game-changer.\n",
    "- Allows analyzing the entire input sequence.\n",
    "- Determines relevance to each component of the output.\n",
    "- Transforms NLP and many other AI domains.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (4.33.2)\n",
      "Requirement already satisfied: filelock in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********\n",
      "Startbucks most popular companies that provides coffee! coffee! coffee! coffee! coffee!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Sample text data\n",
    "text = \"Startbucks is one of the most popular companies that provides coffee!\"\n",
    "\n",
    "# Preprocess the text and create sequences\n",
    "tokens = text.split()\n",
    "word_to_idx = {word: idx for idx, word in enumerate(tokens)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "seq_length = 5\n",
    "\n",
    "data = []\n",
    "for i in range(len(tokens) - seq_length):\n",
    "    seq_in = tokens[i:i + seq_length]\n",
    "    seq_out = tokens[i + seq_length]\n",
    "    data.append((seq_in, seq_out))\n",
    "\n",
    "# Define an RNN-based text generation model\n",
    "class RNNTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNNTextGenerator, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embeddings(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, hidden\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(tokens)\n",
    "embedding_dim = 10\n",
    "hidden_dim = 50\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# Create and train the RNN model\n",
    "model_rnn = RNNTextGenerator(vocab_size, embedding_dim, hidden_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for seq_in, seq_out in data:\n",
    "        seq_in_idx = torch.tensor([word_to_idx[word] for word in seq_in], dtype=torch.long)\n",
    "        seq_out_idx = torch.tensor([word_to_idx[seq_out]], dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hidden = None\n",
    "        for i in range(seq_length):\n",
    "            output, hidden = model_rnn(seq_in_idx[i].view(1, -1), hidden)\n",
    "        \n",
    "        loss = criterion(output.view(1, -1), seq_out_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Generate text using the RNN model\n",
    "seed_text = \"Startbucks\"\n",
    "predicted_text = seed_text\n",
    "hidden = None\n",
    "for _ in range(10):\n",
    "    seq_in_idx = torch.tensor([word_to_idx[word] for word in seed_text.split()], dtype=torch.long)\n",
    "    output, hidden = model_rnn(seq_in_idx[-1].view(1, -1), hidden)\n",
    "    predicted_word_idx = torch.argmax(output).item()\n",
    "    predicted_word = idx_to_word[predicted_word_idx]\n",
    "    predicted_text += \" \" + predicted_word\n",
    "    seed_text += \" \" + predicted_word\n",
    "print(\"********\")\n",
    "print(predicted_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********\n",
      "There are many ways to get around the law.\n",
      "\n",
      "The first is to get a license.\n",
      "\n",
      "The second is to get a license from\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Generate text using the Transformer-based model\n",
    "input_text = \"There are\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(input_ids, max_length=30, num_return_sequences=1)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"********\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transformers Beyond Translation**\n",
    "\n",
    "- State-of-the-art models for numerous NLP tasks.\n",
    "- Recently, Transformers made waves in computer vision.\n",
    "\n",
    "- Impacts on NLP\n",
    "  - Fostered advancements in conversational AI.\n",
    "  - Enabled applications in chatbots, virtual assistants, and more.\n",
    "\n",
    "\n",
    "<img src=\"./images/NLPEvolution.png\"  align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next Word Prediction, Scale, and Fine Tuning — BERT (Google) and GPT (OpenAI) Family — 2018**\n",
    "\n",
    "- AI needed to understand **language beyond translation**.\n",
    "- BERT and GPT addressed this crucial gap.\n",
    "\n",
    "#### Introducing BERT\n",
    "\n",
    "- BERT (Bidirectional Encoder Representations from Transformers).\n",
    "- Google's approach to contextual language understanding.\n",
    "- Trained on vast amounts of text to predict missing words.\n",
    "- BERT's Impact\n",
    "  - Achieved remarkable results in sentiment analysis, question answering, and more.\n",
    "  - Contextual embeddings revolutionized language understanding.\n",
    "\n",
    "#### GPT - A Different Approach\n",
    "\n",
    "- GPT (Generative Pre-trained Transformer) by OpenAI.\n",
    "- Focus on autoregressive language modeling.\n",
    "  - Learning to generate text one word at a time.\n",
    "- GPT's Language Generation\n",
    "  - GPT-2's surprising ability to generate coherent text.\n",
    "  - Human-like responses in chatbots and text generation.\n",
    "  - Demonstrated the power of pre-trained models.\n",
    "\n",
    "\n",
    "<img src=\"./images/GPT.jpeg\" width=\"500\" align=\"center\"/>\n",
    "---\n",
    "\n",
    "#### Scaling Challenges\n",
    "\n",
    "- Collecting quality training data remained a challenge.\n",
    "- ImageNet required meticulous labeling of thousands of images.\n",
    "- Text datasets for language tasks were equally demanding.\n",
    "\n",
    "#### GPT-3: Scaling New Heights\n",
    "\n",
    "- OpenAI introduced GPT-3 with 175 billion parameters.\n",
    "- The largest and most powerful language model to date.\n",
    "\n",
    "#### Fine Tuning - Customizing Models\n",
    "\n",
    "- Fine tuning adapts large models to specific tasks.\n",
    "- Cost-effective compared to training from scratch.\n",
    "- Application in fields like healthcare, finance, and more.\n",
    "- Examples\n",
    "  - Fine-tuned models for medical document processing.\n",
    "  - Improved accuracy in identifying medical conditions.\n",
    "  - OpenAI's partnership with Microsoft for domain-specific AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with and without BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Prediction (TF-IDF + Logistic Regression): Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment (BERT): LABEL_1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "df = pd.read_csv('movie_reviews.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df['review']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Traditional Approach: TF-IDF + Logistic Regression\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "sample_review = [\"The movie was disappointing. The acting was mediocre, and the plot lacked depth. I would not recommend it.\"]\n",
    "\n",
    "# Transform the sample review using TF-IDF\n",
    "sample_review_tfidf = tfidf_vectorizer.transform(sample_review)\n",
    "\n",
    "# Predict the sentiment for the sample review\n",
    "sample_predicted_sentiment_lr = lr_classifier.predict(sample_review_tfidf)\n",
    "sample_sentiment = \"Positive\" if sample_predicted_sentiment_lr[0] == 'positive' else \"Negative\"\n",
    "print(f\"Sentiment Prediction (TF-IDF + Logistic Regression): {sample_sentiment}\")\n",
    "\n",
    "# BERT-based Approach\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "results = nlp(sample_review)\n",
    "predicted_sentiment_bert = results[0]['label']\n",
    "print(f\"Sentiment (BERT): {predicted_sentiment_bert}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Challenge of Interaction**\n",
    "\n",
    "- They focused on predicting the next word.\n",
    "- Interacting with Language Models (LLMs) was challenging.\n",
    "- Difficulties in following human instructions.\n",
    "\n",
    "- **Instruction Tuning Unveiled**\n",
    "  - Fine-tuning LLMs to follow human instructions\n",
    "  - Enhanced interaction and task performance\n",
    "\n",
    "** Benefits of Instruction Tuning**\n",
    "\n",
    "- Increased accuracy and capabilities of LLMs.\n",
    "- Alignment with human values.\n",
    "- Prevention of undesired or dangerous content.\n",
    "\n",
    "\n",
    "## **The Arrival of ChatGPT**\n",
    "\n",
    "- ChatGPT: A milestone in Generative AI.\n",
    "- Reorganized instruction tuning into a dialogue format.\n",
    "- User-friendly interface for AI interaction.\n",
    "\n",
    "\n",
    "## **Popular LLMs**\n",
    "<img src=\"./images/GAI2.webp\" width=\"1000\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OpenAI’s GPT Models**\n",
    "\n",
    "<img src=\"./images/GAI3.webp\" width=\"1000\" align=\"center\"/>\n",
    "\n",
    "## **Task specific**\n",
    "\n",
    "<img src=\"./images/GAI4.webp\" width=\"1000\" align=\"center\"/>\n",
    "\n",
    "\n",
    "<img src=\"./images/GAI5.webp\" width=\"1000\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Google AI and the Pathways Language Model (PaLM)**\n",
    "\n",
    "<img src=\"./images/PALM.jpeg\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "- Google's largest publicly disclosed model.\n",
    "  - PaLM serves as a foundation model.\n",
    "- Used in various Google projects.\n",
    "  - Sscale up to 540 billion parameters.\n",
    "- Trained on 780 billion tokens.\n",
    "- A substantial leap beyond GPT-3.\n",
    "\n",
    "#### Training Data\n",
    "\n",
    "- Self-supervised learning with a diverse text corpus.\n",
    "- Multilingual web pages, books, code repositories, and more.\n",
    "\n",
    "#### PaLM's Performance\n",
    "\n",
    "- PaLM's exceptional few-shot performance.\n",
    "- Outperforming prior larger models like GPT-3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DeepMind’s Chinchilla Model**\n",
    "\n",
    "<img src=\"./images/Deepmind.webp\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "- DeepMind Founded in 2010.\n",
    "- Acquired by Google in 2014, now a subsidiary of Alphabet Inc.\n",
    "  - DeepMind's pursuit of replicating human short-term memory.\n",
    "  - Creation of a Neural Turing Machine.\n",
    "  - A step towards understanding memory in AI.\n",
    "\n",
    "- AlphaZaro\n",
    "  - Competence achieved through reinforcement learning.\n",
    "\n",
    "- AlphaFold's advances in protein folding.\n",
    "  - Predicting over 200 million protein structures.\n",
    "  - Revolutionizing the field of biology.\n",
    "\n",
    "#### Flamingo - Describing Images\n",
    "\n",
    "\n",
    "- In April 2022, DeepMind launched Flamingo.\n",
    "  - A single visual language model capable of describing any picture.\n",
    "  - Advancing AI's understanding of visual content.\n",
    "  - https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model\n",
    "\n",
    "\n",
    "#### Chinchilla AI - Outperforming GPT-3\n",
    "\n",
    "- DeepMind's Chinchilla AI introduced in March 2022.\n",
    "- Outperforming GPT-3.\n",
    "- How \n",
    "  - Chinchilla boasts 70B parameters.\n",
    "  - Trained on 1,400 tokens, 4.7x more than GPT-3.\n",
    "- Significant benefits for inference costs.\n",
    "  - Outperforming other large language model platforms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Meta AI (formerly FAIR)**\n",
    "\n",
    "- FAIR, or Facebook Artificial Intelligence Research.\n",
    "- A laboratory focused on open-source AI frameworks.\n",
    "\n",
    "\n",
    "#### PyText - Advancing NLP\n",
    "\n",
    "- In 2018, FAIR released PyText.\n",
    "- A modeling framework for NLP systems.\n",
    "\n",
    "\n",
    "#### Galactica - Assisting Scientists\n",
    "\n",
    "- November 2022: Meta's Galactica.\n",
    "- Assists scientists with tasks like summarizing papers and annotating molecules.\n",
    "- Bridging the gap between AI and scientific research.\n",
    "\n",
    "\n",
    "## **LLaMA - Large Language Model Meta AI**\n",
    "\n",
    "<img src=\"./images/llama.jpeg\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "- Released in February 2023.\n",
    "- A foundational transformer-based language model.\n",
    "- Aimed at advancing AI research and academic exploration.\n",
    "- Responsible AI\n",
    "  - LLaMA models released under non-commercial licenses.\n",
    "  - Preventing misuse while promoting responsible AI.\n",
    "  - Access granted to select researchers and organizations.\n",
    "- Parameters\n",
    "  - from 7 billion to 65 billion parameters.\n",
    "  - Comparing LLaMA-65B to Chinchilla and PaLM.\n",
    "- Training Data\n",
    "  - LLaMA models trained on 1.4 trillion tokens in 20 languages.\n",
    "  - Leveraging publicly available unlabeled data.\n",
    "  - Data sources include CCNet, GitHub, Wikipedia, ArXiv, Stack Exchange, and books.\n",
    "\n",
    "- Challenges\n",
    "  - LLaMA's performance varies across languages.\n",
    "  - Challenges related to bias, toxicity, and hallucination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Anthropic and the Claude Chatbot**\n",
    "\n",
    "<img src=\"./images/claude.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "- Anthropic: An AI startup and public benefit corporation.\n",
    "- Founded in 2021 by Daniela Amodei and Dario Amodei, former OpenAI members.\n",
    "- A focus on responsible AI and interpretability.\n",
    "\n",
    "\n",
    "#### Claude Chatbot\n",
    "\n",
    "- Introducing Claude, Anthropic's conversational large language model.\n",
    "- Using **constitutional AI** for better alignment with human intentions.\n",
    "- Claude Models\n",
    "  - Claude comes in two versions: Claude-v1 and Claude Instant.\n",
    "  - Claude-v1 for complex dialogues and creative content.\n",
    "  - Claude Instant for casual conversations and summarization.\n",
    "\n",
    "#### Limitations and Concerns\n",
    "\n",
    "- Claude's limitations in math and programming.\n",
    "- Occasional hallucinations and dubious instructions.\n",
    "- Concerns about clever prompting bypassing safety features.\n",
    "\n",
    "**Availability and Integration**\n",
    "\n",
    "- Claude's media embargo lifted in January 2023.\n",
    "- Integration with Discord Juni Tutor Bot and various platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Open Source Efforts in AI and Machine Learning**\n",
    "\n",
    "\n",
    "\n",
    "| Model Family Name | Created By | Sizes | Focus | Foundation or Fine-Tuned | License | What’s Interesting | Architectural Notes |\n",
    "|-------------------|------------|-------|-------|--------------------------|---------|-------------------|--------------------|\n",
    "| LLaMA | Meta | 7B, 13B, 32B, 65.2B | Varied | Foundation | Non-commercial | Basis for numerous fine-tuned variants | SwiGLU activation instead of ReLU |\n",
    "| LLaMA 2 | Meta with Microsoft | 7B, 13B, 70B | Chat | Foundation | Commercial | Balances safety and helpfulness better than OpenAI's models | SwiGLU activation, RoPE over traditional embeddings |\n",
    "| Alpaca | Stanford’s CRFM | 7B | Instruction following | Fine-tuned LLaMA 7B | Non-commercial | Trained on text-davinci-003 examples | - |\n",
    "| Vicuna | LMSYS | 7B, 13B | Chat | Fine-tuned LLaMA 13B | Non-commercial | Utilizes conversations from ShareGPT.com for training | - |\n",
    "| Guanaco | KBlueLeaf | 7B | Instruction following | Fine-tuned LLaMA 7B (parameter efficient) | Non-commercial | Fine-tuned using QLoRA | - |\n",
    "| RedPajama | Multiple collaborators | 3B, 7B | Chat, Instruction following | Foundation | Commercial | Uses the fully open RedPajama dataset following the LLaMA training recipe | Modifications on the Pythia architecture |\n",
    "| Falcon | Technology Innovation Institute of UAE | 7B, 40B | Varied | Foundation | Commercial | Features a 2D parallelism strategy and ZeRo optimization for efficient training | FlashAttention and Multi-query Attention techniques |\n",
    "| Flan-T5 | Google | Various, up to 11B | Varied | Foundation | Commercial | Trained on a massive collection of datasets, tasks, and task categories | Based on the T5 encoder-decoder structure |\n",
    "| Stable Beluga 2 (Freewilly) | Stability AI | 70B | Varied | Fine-tuned LLaMA 2 70B | Non-commercial | Uses a modified Orca approach for high-quality example generation | - |\n",
    "| MPT | MosaicML | Up to 30B | Varied including story writing | Foundation | Commercial | Capable of generating extremely long texts (up to 84k tokens) with specific configurations | Features FlashAttention |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./images/GAI6.png\" width=\"1000\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Image Generation: Dall-E | MidJourney | Stable Diffusion | DreamStudio**\n",
    "\n",
    "\n",
    "<img src=\"./images/GAI7.webp\" width=\"1000\" align=\"center\"/>\n",
    "\n",
    "# AI Innovations in Creativity\n",
    "\n",
    "\n",
    "#### DALL-E\n",
    "\n",
    "**DALL-E** - *By OpenAI*\n",
    "\n",
    "- Generates images from textual descriptions.\n",
    "- Combines \"Dali\" and \"Wall-E.\"\n",
    "- Example: \"a two-story pink house shaped like a shoe.\"\n",
    "- **Limitation:** Limited to predefined concepts, potential for misinterpretation.\n",
    "\n",
    "![DALL-E](images/dalle.webp)\n",
    "\n",
    "\n",
    "#### Midjourney\n",
    "\n",
    "**Midjourney** - *Artificial Intelligence for Exploration*\n",
    "\n",
    "- Enhances exploration in robotics and space missions.\n",
    "- Facilitates autonomous decision-making.\n",
    "- A step towards AI-driven exploration.\n",
    "- **Limitation:** Dependency on data quality, computational resources.\n",
    "\n",
    "![Midjourney](images/midjourney.png)\n",
    "\n",
    "\n",
    "#### Stable Diffusion\n",
    "\n",
    "**Stable Diffusion** - *Generative Model Training*\n",
    "\n",
    "- A technique for training generative models.\n",
    "- Improves stability and quality.\n",
    "- Used in GANs and AI art generation.\n",
    "- **Limitation:** Requires extensive computational power and time.\n",
    "\n",
    "![Stable Diffusion](images/stable.jpeg)\n",
    "\n",
    "\n",
    "#### DreamStudio\n",
    "\n",
    "**DreamStudio** - *AI-Enhanced Creative Tools*\n",
    "\n",
    "- Empowers artists with AI-generated content.\n",
    "- Seamlessly integrates AI into the creative process.\n",
    "- Enables new forms of artistic expression.\n",
    "- **Limitation:** May raise concerns about AI's role in art creation.\n",
    "\n",
    "![DreamStudio](images/dreamstudio.jpeg)\n",
    "\n",
    "\n",
    "# Comparison Table\n",
    "\n",
    "| Innovation     | Description                                | Application                   | Limitation                           | Link                                       |\n",
    "|----------------|--------------------------------------------|--------------------------------|--------------------------------------|--------------------------------------------|\n",
    "| DALL-E         | Generates images from text                | Art, Design, Visual Creativity  | Limited to predefined concepts      | [Link](https://www.openai.com/research/dall-e/) |\n",
    "| Midjourney     | Enhances exploration in robotics           | Space Missions, Autonomous Exploration | Data quality, computational resources | [Link](https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F) |\n",
    "| Stable Diffusion | Training technique for generative models  | Generative Art, GANs          | High computational requirements     | [Link](https://stablediffusionweb.com/) |\n",
    "| DreamStudio    | AI-enhanced creative tools                | Visual Arts, Design, Creativity | Ethical/artistic concerns            | [Link](https://beta.dreamstudio.ai/generate) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Alpaca**\n",
    "\n",
    "- **Model Origin**: Fine-tuned from LLaMA 7B\n",
    "- **Training Data**: 52K instruction-following demonstrations\n",
    "- **Comparison**: Similar behavior to OpenAI’s text-davinci-003\n",
    "- **Cost**: <$600 for reproduction\n",
    "- **Code**: [GitHub.com/Stanford-Alpaca/Alpaca7B](#)\n",
    "\n",
    "- Powerful instruction-following models:\n",
    "    - GPT-3.5 (text-davinci-003)\n",
    "    - ChatGPT\n",
    "    - Claude\n",
    "    - Bing Chat\n",
    "- **Challenges**:\n",
    "    - Generation of false information\n",
    "    - Propagation of stereotypes\n",
    "    - Toxic language generation\n",
    "\n",
    "#### Alpaca Model Details\n",
    "- **Purpose**: Addressing deficiencies in instruction-following models\n",
    "- **Base**: Meta’s LLaMA 7B model\n",
    "- **Training Data**: 52K instructions generated using text-davinci-003\n",
    "- **Behavior**: Similar to text-davinci-003\n",
    "- **Cost**: Surprisingly low\n",
    "\n",
    "\n",
    "#### Training Recipe\n",
    "- **Challenges**:\n",
    "    1. Pretrained language model quality\n",
    "    2. High-quality instruction data\n",
    "- **Solution**: Meta’s new LLaMA models & self-instruct method\n",
    "- **Training Details**: Fine-tuned LLaMA 7B on 52K demonstrations from text-davinci-003\n",
    "- **Data Cost**: <$500 using OpenAI API\n",
    "\n",
    "#### Preliminary Evaluation\n",
    "- **Method**: Human evaluation on self-instruct evaluation set\n",
    "- **Comparison**: Blind pairwise comparison between text-davinci-003 & Alpaca 7B\n",
    "- **Results**: Alpaca and text-davinci-003 had very similar performance\n",
    "- **Demo**: Interactive testing of Alpaca model\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./images/alpaca.jpeg\" width=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recap: Language Models & LLMs**\n",
    "- **Highlight**: Advanced instruction-following models.\n",
    "- **Examples**:\n",
    "    - Alpaca 7B\n",
    "    - ChatGPT \n",
    "    - Claude\n",
    "\n",
    "- **Challenges**: \n",
    "    - Generation of **false information**\n",
    "    - Propagation of **stereotypes**\n",
    "    - Toxic language generation\n",
    "\n",
    "- **Opportunities**: \n",
    "    - Seamless human-computer interaction\n",
    "    - Enhanced content generation\n",
    "    - Automation of complex text-based tasks\n",
    "\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "## **Going back to Generative AI**\n",
    "- **Definition**: AI models designed to **produce** new content.\n",
    "- **Scope**: Beyond text! Think images, videos, music, designs.\n",
    "- **Objective**: Generate content nearly indistinguishable from what humans can produce.\n",
    "\n",
    "#### Key Players in Generative AI\n",
    "1. **Generative Adversarial Networks (GANs)**\n",
    "2. **Variational Autoencoders (VAEs)**\n",
    "\n",
    "\n",
    "## **GANs**\n",
    "- **Concept**: Duel between two networks.\n",
    "    - **Generator**: Crafts fake data.\n",
    "    - **Discriminator**: Sifts real from fake.\n",
    "- **Training Dynamics**: \n",
    "    - Generator crafts better fakes.\n",
    "    - Discriminator refines its discernment.\n",
    "- **Applications**:\n",
    "    - Art creation (e.g., DeepArt)\n",
    "    - Image super-resolution (e.g., SRGAN)\n",
    "    - Generating faces (e.g., NVIDIA's FaceGAN)\n",
    "\n",
    "## **VAEs**\n",
    "- **Philosophy**: Compress data, then rebuild it.\n",
    "- **Key Mechanism**: Introduces randomness during compression.\n",
    "- **Benefits**:\n",
    "    - Structured latent space for generation.\n",
    "    - More consistent generation than GANs.\n",
    "- **Applications**:\n",
    "    - Image denoising\n",
    "    - Content interpolation (e.g., morphing one image into another)\n",
    "    - Generating art with unique styles\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "## **GANs vs VAEs: Quick Comparison**\n",
    "- **Training Stability**:\n",
    "    - GANs can be trickier to train due to the dueling nature.\n",
    "    - VAEs generally offer stable training but might not achieve the same level of detail as GANs.\n",
    "- **Generated Data Quality**:\n",
    "    - GANs often produce sharper images.\n",
    "    - VAEs might produce smoother or blurrier images.\n",
    "- **Applications**:\n",
    "    - GANs dominate in tasks requiring high-resolution outputs.\n",
    "    - VAEs shine where a structured latent space or consistent output is vital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GANs**\n",
    "\n",
    "- Traditional Machine Learning\n",
    "  - Examines a complex input, like an **image**.\n",
    "  - Produces a simple output, like a **label** (\"cat\").\n",
    "\n",
    "- Generative Models\n",
    "  - Takes a small piece of input, perhaps a few random numbers.\n",
    "  - Produces a complex output, like an image of a **realistic-looking face**.\n",
    "\n",
    "- Generative Adversarial Network (GAN)\n",
    "  - An effective type of **generative model**.\n",
    "  - Introduced only a few years ago.\n",
    "\n",
    "- Why Use GANs?\n",
    "  1. **Intellectual Challenge**: Crafting systems that can generate realistic data.\n",
    "  2. **Practical Applications**: From creating art to enhancing blurry images.\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "## **How does a GAN work?**\n",
    "\n",
    "- GANs turn the seemingly impossible goal into reality.\n",
    "- They utilize two key tricks.\n",
    "\n",
    "- Using Randomness as an Ingredient\n",
    "\n",
    "    1. **Variety**: Avoids producing the same output each time.\n",
    "    2. **Mathematical Framework**: Translates image generation into probabilities.\n",
    "    3. **Probability Distribution on Images**: Determines which images are likely to be faces.\n",
    "\n",
    "- Neural Networks and Image Generation\n",
    "  - Modeling a function on a high-dimensional space.\n",
    "  - Neural networks excel at this kind of problem.\n",
    "\n",
    "- The Big Insight: Contest!\n",
    "\n",
    "    - The **\"adversarial\"** part of GAN.\n",
    "    - Two competing networks:\n",
    "    1. **Generator**: Creates random synthetic outputs.\n",
    "    2. **Discriminator**: Distinguishes real outputs from synthetic ones.\n",
    "    \n",
    "- The Adversarial Duel\n",
    "  - The two networks face off.\n",
    "  - They improve together, aiming for a generator that creates realistic outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/GANLab.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "- https://poloclub.github.io/ganlab/\n",
    "  - GANs are complex systems.\n",
    "  - This visualization simplifies GAN mechanics for clarity.\n",
    "\n",
    "\n",
    "- Simplified GAN Mechanics\n",
    "\n",
    "  - Instead of realistic images, the focus is on a **distribution of points in 2D**.\n",
    "  - Easier to understand the mechanics in plain old (x,y) space.\n",
    "\n",
    "- Intruction\n",
    "  - Choose a probability distribution for the GAN to learn.\n",
    "  - Visualized as a **set of data samples**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Variational Autoencoder (VAE) to Generate Images\n",
    "\n",
    "\n",
    "- What is an Autoencoder?\n",
    "  - Transforms input to a **lower dimensional space** (encoding step).\n",
    "  - Reconstructs input from the lower-dimensional representation (decoding step).\n",
    "\n",
    "--  Visual Representation\n",
    "\n",
    "<img src=\"./images/VAE1.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- To generate new images using a VAE, input random vectors to the decoder.\n",
    "\n",
    "<img src=\"./images/VAE2.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- VAE vs Regular Autoencoders\n",
    "\n",
    "  1. Imposes a **probability distribution** on the latent space.\n",
    "  2. Learns the distribution so the outputs match the observed data.\n",
    "  3. Latent outputs are randomly sampled from the distribution learned by the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
